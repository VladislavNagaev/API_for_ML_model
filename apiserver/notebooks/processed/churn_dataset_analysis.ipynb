{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 7.13.0\n",
      "\n",
      "numpy       : 1.22.3\n",
      "pandas      : 1.2.4\n",
      "statsmodels : 0.12.2\n",
      "scikit-learn: 0.24.2\n",
      "tensorflow  : 2.8.0\n",
      "\n",
      "Compiler    : GCC 9.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-109-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 32\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,pandas,statsmodels,scikit-learn,tensorflow -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "import statsmodels.regression.linear_model as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import _pickle as cPickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дирректории рабочих файлов\n",
    "PATH_TO_DATA = r'../../data' \n",
    "\n",
    "# Исходные данные\n",
    "RAW = r'raw'\n",
    "\n",
    "# Директории моделей\n",
    "PATH_TO_MODELS = r'../../models' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding data and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных в формате CSV\n",
    "path_to_csv_file = os.path.join(PATH_TO_DATA, RAW, 'WA_Fn_UseC__Telco_Customer_Churn.csv')\n",
    "\n",
    "full_data = pd.read_csv(\n",
    "    path_to_csv_file, \n",
    "    na_values=[' ','','#NA','NA','NULL','NaN', 'nan', 'n/a'], \n",
    "    dtype={'TotalCharges':np.float32, 'MonthlyCharges': np.float32},\n",
    "    engine='c',\n",
    "    sep=',',\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "# Dropping column not having any significance in predicting the customer decision so we will drop it\n",
    "full_data = full_data.drop(\n",
    "    columns=[\n",
    "        'customerID', \n",
    "        'PaperlessBilling', \n",
    "        'PaymentMethod'\n",
    "    ], \n",
    "    axis=1, \n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Remove na_values\n",
    "full_data.loc[:,'TotalCharges'] = full_data.loc[:,'TotalCharges'].fillna(\n",
    "    full_data.loc[:,'TotalCharges'].mean(), \n",
    "    inplace=False,\n",
    ")\n",
    "\n",
    "\n",
    "target_column = 'Churn'\n",
    "\n",
    "data = full_data.drop(\n",
    "    columns=target_column, \n",
    "    axis=1, \n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "target = full_data.loc[:,[target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the values and feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns having unique values lower than 5\n",
    "# columns_for_encoder = [column for column in data.columns if data.loc[:,column].unique().shape[0] < 5]\n",
    "\n",
    "columns_for_encoder = [\n",
    "    'gender', \n",
    "    'SeniorCitizen', \n",
    "    'Partner', \n",
    "    'Dependents', 'PhoneService', \n",
    "    'MultipleLines', \n",
    "    'InternetService', \n",
    "    'OnlineSecurity', \n",
    "    'OnlineBackup', \n",
    "    'DeviceProtection', \n",
    "    'TechSupport', \n",
    "    'StreamingTV', \n",
    "    'StreamingMovies', \n",
    "    'Contract',\n",
    "]\n",
    "\n",
    "\n",
    "for column in columns_for_encoder:\n",
    "    \n",
    "    # creating label encoders\n",
    "    label_encoder = LabelEncoder().fit(data.loc[:,column]) \n",
    "    \n",
    "    # aplication label encoders\n",
    "    data.loc[:,column] = label_encoder.transform(data.loc[:,column])\n",
    "    \n",
    "    # saving label encoders\n",
    "    label_encoder_name = f'label_encoder_{column}'\n",
    "    label_encoder_path = os.path.join(PATH_TO_MODELS, label_encoder_name + '.pkl')\n",
    "    with open(label_encoder_path, 'wb') as fid:\n",
    "        cPickle.dump(label_encoder, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = target_column\n",
    "\n",
    "\n",
    "# creating label encoders\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.array(['No', 'Yes'])\n",
    "\n",
    "# aplication label encoders\n",
    "target.loc[:,column] = label_encoder.transform(target.loc[:,column])\n",
    "    \n",
    "# saving label encoders\n",
    "label_encoder_name = f'label_encoder_{column}'\n",
    "label_encoder_path = os.path.join(PATH_TO_MODELS, label_encoder_name + '.pkl')\n",
    "with open(label_encoder_path, 'wb') as fid:\n",
    "    cPickle.dump(label_encoder, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_for_scaler = [\n",
    "    'gender',\n",
    "    'SeniorCitizen',\n",
    "    'Partner',\n",
    "    'Dependents',\n",
    "    'tenure',\n",
    "    'PhoneService',\n",
    "    'MultipleLines',\n",
    "    'InternetService',\n",
    "    'OnlineSecurity',\n",
    "    'OnlineBackup',\n",
    "    'DeviceProtection',\n",
    "    'TechSupport',\n",
    "    'StreamingTV',\n",
    "    'StreamingMovies',\n",
    "    'Contract',\n",
    "    'MonthlyCharges',\n",
    "    'TotalCharges'\n",
    "]\n",
    "\n",
    "\n",
    "for column in columns_for_scaler:\n",
    "    \n",
    "    # creating Feature Scaling\n",
    "    feature_scaler = StandardScaler().fit(data.loc[:,column].values.reshape(-1,1))\n",
    "    \n",
    "    # aplication Feature Scaling\n",
    "    data.loc[:,column] = feature_scaler.transform(data.loc[:,column].values.reshape(-1,1))  \n",
    "    \n",
    "    # saving Feature Scaling\n",
    "    feature_scaler_name = f'feature_scaler_{column}'\n",
    "    feature_scaler_path = os.path.join(PATH_TO_MODELS, feature_scaler_name + '.pkl')\n",
    "    with open(feature_scaler_path, 'wb') as fid:\n",
    "        cPickle.dump(feature_scaler, fid)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing multi-collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance inflation factor is a measure for the increase of the\n",
    "variance of the parameter estimates if an additional variable, given by\n",
    "exog_idx is added to the linear regression. It is a measure for\n",
    "multicollinearity of the design matrix, exog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif_(data, thresh=5.0):\n",
    "    variables = list(data.columns)\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(data.loc[:, variables].values, i) for i, _ in enumerate(variables)]\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            variables.remove(variables[maxloc])\n",
    "            dropped = True\n",
    "    return variables\n",
    "\n",
    "\n",
    "variables_list = calculate_vif_(data, 5)\n",
    "\n",
    "\n",
    "# variables_list = [\n",
    "#     'gender',\n",
    "#     'SeniorCitizen',\n",
    "#     'Partner',\n",
    "#     'Dependents',\n",
    "#     'tenure',\n",
    "#     'PhoneService',\n",
    "#     'MultipleLines',\n",
    "#     'InternetService',\n",
    "#     'OnlineSecurity',\n",
    "#     'OnlineBackup',\n",
    "#     'DeviceProtection',\n",
    "#     'TechSupport',\n",
    "#     'StreamingTV',\n",
    "#     'StreamingMovies',\n",
    "#     'Contract',\n",
    "#     'MonthlyCharges',\n",
    "# ]\n",
    "\n",
    "data = data.loc[:,variables_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building optimal model using backward elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.<br>\n",
    " The p-value is a number between 0 and 1 and interpreted in the following way: A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.<br>\n",
    "A null hypothesis is a type of hypothesis used in statistics that proposes that no statistical significance exists in a set of given observations. The null hypothesis attempts to show that no variation exists between variables or that a single variable is no different than its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardElimination(data, target, SL:int=0.05):\n",
    "    \n",
    "    wdata = data.copy()\n",
    "    \n",
    "    wdata.insert(loc=0, column='A', value=1)\n",
    "\n",
    "    temp = pd.DataFrame(0, index=np.arange(wdata.shape[0]), columns=wdata.columns)\n",
    "    \n",
    "    numVars = wdata.shape[1]\n",
    "    \n",
    "    for i in range(numVars):\n",
    "        \n",
    "        regressor_OLS = sm.OLS(target.values, wdata.values).fit()\n",
    "        \n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        \n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        \n",
    "        if maxVar > SL:\n",
    "            \n",
    "            for j in range(numVars-i):\n",
    "                \n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    \n",
    "                    temp.iloc[:,j] = wdata.iloc[:, j]\n",
    "                    wdata = wdata.drop(\n",
    "                        columns=[wdata.columns[j]], \n",
    "                        inplace=False\n",
    "                    )                    \n",
    "                    \n",
    "                    tmp_regressor = sm.OLS(target.values, wdata.values).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    \n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        \n",
    "                        x_rollback = pd.concat([wdata, temp.iloc[:,[0,j]]], axis=1, join=\"inner\")\n",
    "                        x_rollback = x_rollback.drop(\n",
    "                            columns=[x_rollback.columns[j]], \n",
    "                            inplace=False\n",
    "                        )\n",
    "                        \n",
    "                        x_rollback = x_rollback.drop(\n",
    "                            columns=['A'], \n",
    "                            inplace=False\n",
    "                        )\n",
    "                        \n",
    "                        print (regressor_OLS.summary())\n",
    "                        \n",
    "                        return x_rollback\n",
    "                    \n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "    regressor_OLS.summary()\n",
    "    \n",
    "    wdata = wdata.drop(\n",
    "        columns=['A'], \n",
    "        inplace=False\n",
    "    )            \n",
    "    \n",
    "    return wdata.columns\n",
    "\n",
    "\n",
    "modeled_columns = backwardElimination(data=data, target=target, SL=0.05)\n",
    "\n",
    "\n",
    "\n",
    "# modeled_columns = [\n",
    "#     'SeniorCitizen',\n",
    "#     'Dependents',\n",
    "#     'tenure',\n",
    "#     'PhoneService',\n",
    "#     'MultipleLines',\n",
    "#     'InternetService',\n",
    "#     'OnlineSecurity',\n",
    "#     'OnlineBackup',\n",
    "#     'DeviceProtection',\n",
    "#     'TechSupport',\n",
    "#     'Contract',\n",
    "#     'MonthlyCharges'\n",
    "# ]\n",
    "\n",
    "data = data.loc[:,modeled_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Splitting Dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    data, target, \n",
    "    test_size=0.4, \n",
    "    random_state=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.<br><br>\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can easily observe cluster of red dots i.e. Churn 'Yes' and blue dot i.e. Churn 'No'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устройства, доступные для вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14981125403561195719\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2257256448\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12846258848837192699\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:09:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание последовательной модели\n",
    "model = Sequential([\n",
    "    Dense(\n",
    "        units=64, \n",
    "        activation='relu', \n",
    "        input_shape=(train_data.shape[1],), \n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "    ),\n",
    "    Dense(\n",
    "        units=128, \n",
    "        activation='relu', \n",
    "        input_shape=(train_data.shape[1],), \n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    ),\n",
    "    Dense(\n",
    "        units=64, \n",
    "        activation='relu', \n",
    "        input_shape=(train_data.shape[1],),\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    ),\n",
    "    Dense(\n",
    "        units=32, \n",
    "        activation='relu', \n",
    "        input_shape=(train_data.shape[1],), \n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    ),\n",
    "    Dense(\n",
    "        units=1, \n",
    "        activation = 'sigmoid',\n",
    "    )\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(\n",
    "    optimizer=Adam(), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,521\n",
      "Trainable params: 19,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Параметры скомпилированной сети\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "133/133 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.7680 - val_loss: 0.4447 - val_accuracy: 0.7793\n",
      "Epoch 2/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.7955 - val_loss: 0.4431 - val_accuracy: 0.7747\n",
      "Epoch 3/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8092 - val_loss: 0.4422 - val_accuracy: 0.7903\n",
      "Epoch 4/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8038 - val_loss: 0.4380 - val_accuracy: 0.7881\n",
      "Epoch 5/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8097 - val_loss: 0.4380 - val_accuracy: 0.7864\n",
      "Epoch 6/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8123 - val_loss: 0.4424 - val_accuracy: 0.7871\n",
      "Epoch 7/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8137 - val_loss: 0.4493 - val_accuracy: 0.7846\n",
      "Epoch 8/500\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8144 - val_loss: 0.4426 - val_accuracy: 0.7832\n",
      "CPU times: user 3.54 s, sys: 261 ms, total: 3.8 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Обучение сети\n",
    "np.random.seed(42)\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    history = model.fit(\n",
    "        # Данные для обучения\n",
    "        train_data.values, train_target.values,\n",
    "        # Размер мини-выборки\n",
    "        batch_size=32, \n",
    "        # Количество эпох для обучения\n",
    "        epochs=500,\n",
    "        # Валидационная выборка\n",
    "        validation_data=(test_data.values, test_target.values), \n",
    "        # Необходимость перемешивания данных\n",
    "        shuffle=True,\n",
    "        # Уровень вывода данных о процессе обучения\n",
    "        verbose=1,\n",
    "        initial_epoch=0,\n",
    "        # Создание EarlyStopping Callback для остановки обучения сети в случае переобучения\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                # Отслеживаемая на каждой эпохе метрика\n",
    "                monitor='accuracy', \n",
    "                # Терпение - количество эпох, на которых может не быть улучшения выбранной метрики \n",
    "                # до ранней остановки\n",
    "                patience=5\n",
    "            ), \n",
    "            EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Training***\n",
      "accuracy:  0.8210650887573965\n",
      "precision:  0.6955974842767295\n",
      "recall:  0.5182755388940956\n",
      "***Testing***\n",
      "accuracy:  0.783179559971611\n",
      "precision:  0.6666666666666666\n",
      "recall:  0.4763092269326683\n"
     ]
    }
   ],
   "source": [
    "print(\"***Training***\")\n",
    "train_target_pred = (model.predict(train_data.values) >= 0.5).astype(np.int32).reshape(-1,)\n",
    "print(\"accuracy: \", accuracy_score(train_target.values,train_target_pred))\n",
    "print(\"precision: \", precision_score(train_target.values,train_target_pred))\n",
    "print(\"recall: \", recall_score(train_target.values,train_target_pred))\n",
    "\n",
    "print(\"***Testing***\")\n",
    "test_target_pred = (model.predict(test_data.values) >= 0.5).astype(np.int32).reshape(-1,)\n",
    "print(\"accuracy: \", accuracy_score(test_target.values,test_target_pred))\n",
    "print(\"precision: \", precision_score(test_target.values,test_target_pred))\n",
    "print(\"recall: \", recall_score(test_target.values,test_target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'model_Sequential'\n",
    "model_path = os.path.join(PATH_TO_MODELS, model_name + '.h5')\n",
    "\n",
    "model.save(filepath=model_path, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 819.69966,
   "position": {
    "height": "40px",
    "left": "1640px",
    "right": "20px",
    "top": "129px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
